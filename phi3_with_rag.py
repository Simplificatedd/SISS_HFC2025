# -*- coding: utf-8 -*-
"""SISS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-qzjtYtEydOeiXqMPJtEb9DcRsz8jieQ

### Install Dependencies
"""

# !pip install pdfplumber
# !pip install os
# !pip install sentence_transformers
# !pip install numpy
# !pip install faiss-cpu

# """### Install Ollama and Langchain"""

# !curl -fsSL https://ollama.com/install.sh | sh
# !pip install langchain_community

# """### Start Ollama server as a background process and pull model image for phi3"""

# import subprocess
# process = subprocess.Popen(["ollama", "serve"])

# !ollama list

# !ollama pull phi3

"""### RAG"""

import os
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from langchain_community.llms import Ollama

# Define file paths for the CSV datasets
CAREERS_CSV_PATH = "/content/sample_data/mycareersfuture_jobs.csv"
SKILLS_CSV_PATH = "/content/sample_data/skillsfuture_courses.csv"

# Load data from CSV files
def load_csv_data(csv_path):
    return pd.read_csv(csv_path)

# Step 1: Extract text from CSV data for embeddings
def extract_text_from_csv(data, text_columns):
    combined_text = data[text_columns].fillna("").apply(" ".join, axis=1)
    return combined_text.tolist()

# Step 2: Chunk the text for efficient retrieval (optional for large datasets)
def chunk_text(text_list, chunk_size=1000, overlap=200):
    chunks = []
    for text in text_list:
        for i in range(0, len(text), chunk_size - overlap):
            chunks.append(text[i:i + chunk_size])
    return chunks

# Step 3: Create embeddings for each chunk using SentenceTransformer
def create_embeddings(chunks):
    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(chunks, convert_to_tensor=False)
    return np.array(embeddings)

# Step 4: Build FAISS index
def build_faiss_index(embeddings):
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index

# Step 5: Search for relevant chunks in FAISS index
def search_faiss_index(query, faiss_index, chunks, model):
    query_embedding = model.encode([query], convert_to_tensor=False)
    D, I = faiss_index.search(np.array(query_embedding), k=5)
    relevant_chunks = [chunks[i] for i in I[0]]
    return relevant_chunks

# Load and process datasets
careers_data = load_csv_data(CAREERS_CSV_PATH)
skills_data = load_csv_data(SKILLS_CSV_PATH)

# Specify columns to use for embeddings
careers_text_columns = ["Job Title", "Company", "Location", "Employment Type", "Salary"]
skills_text_columns = ["Institution", "Course Title", "Upcoming Date", "Duration", "Training Mode", "Full Fee", "Funded Fee"]

# Extract text and create embeddings for MyCareersFuture
careers_text = extract_text_from_csv(careers_data, careers_text_columns)
careers_chunks = chunk_text(careers_text)
careers_embeddings = create_embeddings(careers_chunks)
careers_faiss_index = build_faiss_index(careers_embeddings)

# Extract text and create embeddings for SkillsFuture
skills_text = extract_text_from_csv(skills_data, skills_text_columns)
skills_chunks = chunk_text(skills_text)
skills_embeddings = create_embeddings(skills_chunks)
skills_faiss_index = build_faiss_index(skills_embeddings)

print("Data processing and FAISS index creation complete.")


# CO-STAR Framework Components
context = """
You are an AI assistant designed to answer questions about job opportunities from MyCareersFuture and courses from SkillsFuture. Your goal is to provide accurate, relevant, and concise answers based on the datasets.

If a question falls outside the domain of jobs or courses, politely respond with: "I only answer questions related to jobs and courses from MyCareersFuture and SkillsFuture." Always maintain a professional and educational tone.
"""

outcome = """1. If the user asks a question about jobs or courses, focus on providing relevant information by extracting context from the datasets.
2. Include useful links where the user can find detailed information about jobs or courses.
3. Conclude with a follow-up question that encourages exploration of related opportunities or courses."""

scale = """Adapt responses based on the complexity of the user's queries. For beginners, provide straightforward answers. For advanced users, include additional insights such as job market trends or course recommendations."""

time = """Keep each response concise and to the point. Provide enough information to answer the user's question but avoid overwhelming them with unnecessary details."""

actor = """You, the AI assistant, act as a guide and advisor. Engage with users by asking follow-up questions, providing clarifications, and offering actionable recommendations."""

resources = """Utilize the MyCareersFuture and SkillsFuture datasets, along with FAISS for retrieval and SentenceTransformer for embeddings."""


def answer_question(query, history, model_name, faiss_index, chunks, link_column, data):
    llm = Ollama(model=model_name)
    MAX_HISTORY_LENGTH = 10
    history.append(("User", query))

    if len(history) > MAX_HISTORY_LENGTH:
        history = history[-MAX_HISTORY_LENGTH:]

    try:
        model = SentenceTransformer("all-MiniLM-L6-v2")
        relevant_chunks = search_faiss_index(query, faiss_index, chunks, model)

        # Retrieve links corresponding to the chunks
        links = []
        for chunk in relevant_chunks:
            idx = chunks.index(chunk)
            links.append(data.iloc[idx][link_column])

        combined_chunks = "\n".join(relevant_chunks)

        # Create a prompt for the chatbot
        prompt = (
            context + outcome + scale + time + actor + resources +
            "\nConversation History:\n" + "\n".join([f"{sender}: {message}" for sender, message in history]) +
            "\n\nRelevant Context:\n" + combined_chunks +
            "\n\nRelevant Links:\n" + "\n".join(links) +
            "\n\nQuestion: " + query
        )

        response = llm(prompt)
        history.append(("Chatbot", response))
        if len(history) > MAX_HISTORY_LENGTH:
            history = history[-MAX_HISTORY_LENGTH:]

        return history, response

    except Exception as e:
        return history, f"Error generating response: {str(e)}"

if __name__ == "__main__":
    user_query = "What jobs are available in the IT sector?"
    history = []
    model_name = "phi3"

    # Use MyCareersFuture FAISS index
    history, response = answer_question(
        user_query,
        history,
        model_name,
        careers_faiss_index,
        careers_chunks,
        link_column="Link",
        data=careers_data
    )
    print(response)

    user_query = "What courses are available for data analytics?"
    history, response = answer_question(
        user_query,
        history,
        model_name,
        skills_faiss_index,
        skills_chunks,
        link_column="Link",
        data=skills_data
    )
    print(response)